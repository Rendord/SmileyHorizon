Embed the emoji names

group the embeddings to create categories

the categories could be things like (faces, hand gestures, animals) 

we could be done here but we

we want to Label the categories to understand the clusters

this also allows us to verify our approach

To do this we use a local LLM running mistral on ollama

we feed it 5-10 random emojis from a cluster and ask it to summarize the category

Mistral is a bit headstrong so getting the LLM to reduce the output, required a lot repetitive prompting

We then clean the output to produce consistent labels for our clusters

One possible improvement would have been a better selection to the representation, picking examples based on their diversity within the cluster iso randomly
Another improvement could have been finding clusters dynamically instead of predetermining the amount of clusters
But you have to weight the cost of investment vs the eventual quality

(Step 2: Grouping Similar Names with Clustering)
* Explain the goal: Group the embedding vectors so that names with similar meanings are in the same cluster.
* Mention the clustering algorithm used (e.g., K-Means, HDBSCAN).
* Explain what the clusters represent (e.g., one cluster might contain various names for faces, another for animals, another for objects).
* [Optional: Show a visualization of clusters if possible, or describe example clusters].
(Step 3: Understanding the Clusters with an LLM)
* Explain the goal: Get a human-readable summary of what each cluster represents to help in selecting representative names.
* Introduce Ollama (or the specific LLM used locally).
* Describe the prompt: How did you ask the LLM to summarize a cluster of names? (e.g., "Here are some emoji names: [...]. What is the main theme or category of these emojis?").
* Explain the value: The LLM helps you quickly understand the semantic space covered by your emoji names and validate the clustering.
(Step 4: Selecting the Final Payload)
* Explain the strategy for choosing names from the clusters to create the final payload.
* Examples:
    * Picking one or two representative names from each cluster.
    * Prioritizing names from larger clusters.
    * Manually reviewing cluster contents and selecting diverse examples.
* State the size of the final, reduced payload.
(The Outcome: Bug Squashed (and a Cool Tool Built))
* Confirm that adding the template with the smaller, representative payload fixed the specific bug described in the Radar.
* Mention the satisfaction of solving it with a data-driven approach.
* Briefly touch on the "run off the mill but pretty cool" aspect â€“ while embeddings/clustering/LLMs are common, applying them pragmatically to a specific NLU problem like this is a neat trick.
(Reflections and Future Thoughts)
* What did you learn from this?
* How else could this approach be used? (e.g., cleaning other messy lists, generating smaller test sets, understanding the semantic coverage of existing templates).
* Any challenges faced during the process?
* End with a forward-looking statement about working on NLU/AI features.
(Closing)
* A brief thank you for reading